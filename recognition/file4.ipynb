{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "import random\n",
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"./datasets/oxford_pets/images/images/\"\n",
    "target_dir = \"./datasets/oxford_pets/annotations/annotations/trimaps/\"\n",
    "img_siz = (160, 160) # 모델에 입력되는 영상 크기\n",
    "n_class = 3 # 분할 레이블 (1:물체, 2:배경, 3:경계)\n",
    "batch_siz = 32 # 미니 배치 크기\n",
    "\n",
    "img_paths = sorted([os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith(\".jpg\")])\n",
    "label_paths = sorted([os.path.join(target_dir, f) for f in os.listdir(target_dir) if f.endswith(\".png\") and not f.startswith('.')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OxfordPets(keras.utils.Sequence): # 데이터셋을 읽어오는 클래스\n",
    "    def __init__(self, batch_size, img_size, img_paths, label_paths):\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.img_paths = img_paths\n",
    "        self.label_paths = label_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_paths)//self.batch_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        i = idx*self.batch_size\n",
    "        batch_img_paths = self.img_paths[i:i+self.batch_size]\n",
    "        batch_label_paths = self.label_paths[i:i+self.batch_size]\n",
    "        x = np.zeros((self.batch_size,) + self.img_size + (3, ), dtype = \"float32\")\n",
    "        for j, path in enumerate(batch_img_paths):\n",
    "            img = load_img(path, target_size=self.img_size)\n",
    "            x[j] = img\n",
    "        y = np.zeros((self.batch_size,) + self.img_size + (1, ), dtype = \"uint8\")\n",
    "        for j, path in enumerate(batch_label_paths):\n",
    "            img = load_img(path, target_size=self.img_size, color_mode=\"grayscale\")\n",
    "            y[j] = np.expand_dims(img, 2)\n",
    "            y[j] -= 1 # 부류 번호를 1, 2, 3에서 0, 1, 2로 변환\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(img_size, num_classes): # U-Net 모델을 만드는 클래스\n",
    "    inputs = keras.Input(shape=img_size+(3, ))\n",
    "\n",
    "    # U-Net의 다운 샘플링(축소 경로)\n",
    "    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    previous_block_activation = x # 지름길 연결을 위해\n",
    "\n",
    "    for filters in [64, 128, 256]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\")(previous_block_activation)\n",
    "        x = layers.add([x, residual]) # 지름길 연결\n",
    "        previous_block_activation = x # 지름길 연결을 위해\n",
    "\n",
    "    # U-Net의 업 샘플링(확대 경로)\n",
    "    for filters in [256, 128, 64, 32]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.UpSampling2D(2)(x)\n",
    "        residual = layers.UpSampling2D(2)(previous_block_activation)\n",
    "        residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n",
    "        x = layers.add([x, residual]) # 지름길 연결\n",
    "        previous_block_activation = x # 지름길 연결을 위해\n",
    "\n",
    "    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n",
    "    model = keras.Model(inputs, outputs) # 모델 생성\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AppleMint\\AppData\\Local\\anaconda3\\envs\\test\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m585s\u001b[0m 3s/step - accuracy: 0.6800 - loss: 1.3134 - val_accuracy: 0.5726 - val_loss: 4.3765\n",
      "Epoch 2/30\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m561s\u001b[0m 3s/step - accuracy: 0.7846 - loss: 0.5345 - val_accuracy: 0.5726 - val_loss: 4.3295\n",
      "Epoch 3/30\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m562s\u001b[0m 3s/step - accuracy: 0.8172 - loss: 0.4580 - val_accuracy: 0.7278 - val_loss: 0.7586\n",
      "Epoch 4/30\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m560s\u001b[0m 3s/step - accuracy: 0.8378 - loss: 0.4061 - val_accuracy: 0.7910 - val_loss: 0.5408\n",
      "Epoch 5/30\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m562s\u001b[0m 3s/step - accuracy: 0.8509 - loss: 0.3756 - val_accuracy: 0.7971 - val_loss: 0.5185\n",
      "Epoch 6/30\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m561s\u001b[0m 3s/step - accuracy: 0.8655 - loss: 0.3384 - val_accuracy: 0.8403 - val_loss: 0.4104\n",
      "Epoch 7/30\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m561s\u001b[0m 3s/step - accuracy: 0.8720 - loss: 0.3222 - val_accuracy: 0.8362 - val_loss: 0.4184\n",
      "Epoch 8/30\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m562s\u001b[0m 3s/step - accuracy: 0.8823 - loss: 0.2956 - val_accuracy: 0.8392 - val_loss: 0.4402\n",
      "Epoch 9/30\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m591s\u001b[0m 3s/step - accuracy: 0.8908 - loss: 0.2725 - val_accuracy: 0.8388 - val_loss: 0.4339\n",
      "Epoch 10/30\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m571s\u001b[0m 3s/step - accuracy: 0.8966 - loss: 0.2575 - val_accuracy: 0.8500 - val_loss: 0.4203\n",
      "Epoch 11/30\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m562s\u001b[0m 3s/step - accuracy: 0.9028 - loss: 0.2397 - val_accuracy: 0.8401 - val_loss: 0.4541\n",
      "Epoch 12/30\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m569s\u001b[0m 3s/step - accuracy: 0.9001 - loss: 0.2468 - val_accuracy: 0.8493 - val_loss: 0.4303\n",
      "Epoch 13/30\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m583s\u001b[0m 3s/step - accuracy: 0.9131 - loss: 0.2141 - val_accuracy: 0.8544 - val_loss: 0.4114\n",
      "Epoch 14/30\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m641s\u001b[0m 3s/step - accuracy: 0.9173 - loss: 0.2016 - val_accuracy: 0.8474 - val_loss: 0.4492\n",
      "Epoch 15/30\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m630s\u001b[0m 3s/step - accuracy: 0.9197 - loss: 0.1952 - val_accuracy: 0.8527 - val_loss: 0.4399\n",
      "Epoch 16/30\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m571s\u001b[0m 3s/step - accuracy: 0.9245 - loss: 0.1826 - val_accuracy: 0.8238 - val_loss: 0.5591\n",
      "Epoch 17/30\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m569s\u001b[0m 3s/step - accuracy: 0.9171 - loss: 0.2020 - val_accuracy: 0.8401 - val_loss: 0.5083\n",
      "Epoch 18/30\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m623s\u001b[0m 3s/step - accuracy: 0.9239 - loss: 0.1838 - val_accuracy: 0.8611 - val_loss: 0.4328\n",
      "Epoch 19/30\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m586s\u001b[0m 3s/step - accuracy: 0.9358 - loss: 0.1529 - val_accuracy: 0.8632 - val_loss: 0.4600\n",
      "Epoch 20/30\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m604s\u001b[0m 3s/step - accuracy: 0.9415 - loss: 0.1387 - val_accuracy: 0.8595 - val_loss: 0.5060\n",
      "Epoch 21/30\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m608s\u001b[0m 3s/step - accuracy: 0.9430 - loss: 0.1359 - val_accuracy: 0.8609 - val_loss: 0.5269\n",
      "Epoch 22/30\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m628s\u001b[0m 3s/step - accuracy: 0.9428 - loss: 0.1356 - val_accuracy: 0.8513 - val_loss: 0.5557\n",
      "Epoch 23/30\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m636s\u001b[0m 3s/step - accuracy: 0.9243 - loss: 0.1846 - val_accuracy: 0.8381 - val_loss: 0.5076\n",
      "Epoch 24/30\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m606s\u001b[0m 3s/step - accuracy: 0.9282 - loss: 0.1733 - val_accuracy: 0.8569 - val_loss: 0.4612\n",
      "Epoch 25/30\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m609s\u001b[0m 3s/step - accuracy: 0.9406 - loss: 0.1416 - val_accuracy: 0.8635 - val_loss: 0.4849\n",
      "Epoch 26/30\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m612s\u001b[0m 3s/step - accuracy: 0.9493 - loss: 0.1194 - val_accuracy: 0.8622 - val_loss: 0.5150\n",
      "Epoch 27/30\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m611s\u001b[0m 3s/step - accuracy: 0.9511 - loss: 0.1162 - val_accuracy: 0.8671 - val_loss: 0.5340\n",
      "Epoch 28/30\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m579s\u001b[0m 3s/step - accuracy: 0.9553 - loss: 0.1049 - val_accuracy: 0.8665 - val_loss: 0.5526\n",
      "Epoch 29/30\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m589s\u001b[0m 3s/step - accuracy: 0.9563 - loss: 0.1026 - val_accuracy: 0.8579 - val_loss: 0.5566\n",
      "Epoch 30/30\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m628s\u001b[0m 3s/step - accuracy: 0.9357 - loss: 0.1566 - val_accuracy: 0.8365 - val_loss: 0.5263\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 785ms/step\n"
     ]
    }
   ],
   "source": [
    "model = make_model(img_siz, n_class) # 모델 생성\n",
    "\n",
    "random.Random(1).shuffle(img_paths)\n",
    "random.Random(1).shuffle(label_paths)\n",
    "test_samples = int(len(img_paths)*0.1) # 10%를 테스트 집합으로 사용\n",
    "train_img_paths = img_paths[:-test_samples]\n",
    "train_label_paths = label_paths[:-test_samples]\n",
    "test_img_paths = img_paths[-test_samples:]\n",
    "test_label_paths = label_paths[-test_samples:]\n",
    "\n",
    "train_gen = OxfordPets(batch_siz, img_siz, train_img_paths, train_label_paths) # 훈련 집합\n",
    "test_gen = OxfordPets(batch_siz, img_siz, test_img_paths, test_label_paths) # 검증 집합\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "cb = [keras.callbacks.ModelCheckpoint(\"oxford_seg.keras\", save_best_only=True)] # 학습 결과 자동 저장 # 체크포인트 기능을 이용하여 학습 도중에 모델을 저장, 학습 도중에 발생한 가장 높은 성능의 모델만 기록\n",
    "\n",
    "model.fit(train_gen, epochs=30, validation_data=test_gen, callbacks=cb)\n",
    "\n",
    "preds = model.predict(test_gen) # 예측\n",
    "\n",
    "cv.imshow(\"Sample image\", cv.imread(test_img_paths[0])) # 0번 영상 디스플레이\n",
    "cv.imshow(\"Segmentation label\", cv.imread(test_label_paths[0])*64)\n",
    "cv.imshow(\"Segmentation prediction\", preds[0]) # 0번 영상 예측 결과 디스플레이\n",
    "\n",
    "cv.waitKey()\n",
    "cv.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
